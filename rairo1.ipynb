{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete TTS Fine-Tuning Notebook\n",
    "\n",
    "This notebook performs the following steps:\n",
    "\n",
    "1. **Data Loading and Preprocessing:** Reads two TSV metadata files (each with 250 samples) corresponding to the audio directories `pleshy_1` and `pleshy_3`, creates a combined dataset, normalizes the text, and constructs the full audio file paths.\n",
    "2. **Fine-Tuning:** Loads a baseline TTS model (here, using a hypothetical model `facebook/mms-tts-en` from HuggingFace), tokenizes the text data, and fine-tunes the model using HuggingFaceâ€™s Trainer API.\n",
    "3. **Inference:** Uses the fine-tuned model to generate audio (WAV files) for a set of example sentences.\n",
    "4. **Evaluation:** Outlines an evaluation strategy (objective and subjective measures).\n",
    "\n",
    "Reference: Exercise on TTS.pdf](file-service://file-R7VWcf23C2EkjC6TSfP8mB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c40cf9-8a87-44d5-9f79-66ddf8a4d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup: Install required packages\n",
    "!pip install transformers datasets torchaudio TTS\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "print('Environment setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28134246-c227-4a09-a887-8d64ffb8f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TSV metadata files for each folder\n",
    "# Assume each TSV has columns like 'filename' and 'text'\n",
    "metadata1 = pd.read_csv('recorder1.tsv', sep='\\t')\n",
    "metadata3 = pd.read_csv('recorder3.tsv', sep='\\t')\n",
    "\n",
    "print('Metadata from recorder1.tsv:')\n",
    "print(metadata1.head())\n",
    "\n",
    "print('Metadata from recorder3.tsv:')\n",
    "print(metadata3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fbfc8d-2b3d-4c4e-9d0a-9736c23eab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the full audio file path for each sample\n",
    "# For metadata from recorder1.tsv, files are in folder 'pleshy_1'; for recorder3.tsv, in folder 'pleshy_3'\n",
    "metadata1['audio_filepath'] = metadata1['filename'].apply(lambda x: os.path.join('pleshy_1', x))\n",
    "metadata3['audio_filepath'] = metadata3['filename'].apply(lambda x: os.path.join('pleshy_3', x))\n",
    "\n",
    "# Combine the two metadata DataFrames\n",
    "combined_metadata = pd.concat([metadata1, metadata3], ignore_index=True)\n",
    "\n",
    "print('Combined metadata:')\n",
    "print(combined_metadata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a2079b-9c27-45e4-b6e5-fd6a2a5d5b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization function (example: lowercasing)\n",
    "def normalize_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Apply normalization\n",
    "combined_metadata['text'] = combined_metadata['text'].apply(normalize_text)\n",
    "\n",
    "# Reorder columns if needed (ensure 'audio_filepath' and 'text' exist)\n",
    "combined_metadata = combined_metadata[['audio_filepath', 'text']]\n",
    "\n",
    "# Save the combined metadata as a CSV file (using comma delimiter for HuggingFace dataset loading)\n",
    "combined_metadata.to_csv('combined_metadata.csv', index=False)\n",
    "print('Combined metadata saved as combined_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5f528-23f2-44fa-94f9-52a8e3a1ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined metadata as a dataset using HuggingFace Datasets\n",
    "dataset = load_dataset('csv', data_files={'train': 'combined_metadata.csv'})\n",
    "\n",
    "print('Dataset loaded:')\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32d95d-5e9d-4cc9-8e57-0f2d223e9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine-Tuning the Baseline TTS Model\n",
    "\n",
    "# Load the pre-trained TTS model and tokenizer\n",
    "model_name = 'facebook/mms-tts-en'  # Replace with the actual TTS model if different\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print('Loaded TTS model and tokenizer.')\n",
    "\n",
    "# Define a preprocessing function to tokenize the text\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the text; adjust parameters as needed for your TTS model\n",
    "    inputs = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "    # You can add additional processing steps (e.g., audio feature extraction) if required by your model\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./tts_finetuned',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train']\n",
    ")\n",
    "\n",
    "print('Starting fine-tuning...')\n",
    "trainer.train()\n",
    "print('Fine-tuning complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d713df1-cb86-48b7-8d76-6c1a8b5c65c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference: Generating WAV Files\n",
    "\n",
    "# List of example sentences to synthesize\n",
    "sentences = [\n",
    "    \"Voluntary participation of citizens in social groups, networks and social transformation\",\n",
    "    \"Later on black eye, Vikings, Mafia, Black Beret, daughters of jezebel\",\n",
    "    \"Statutory instruments: These are known as ministerial orders or departmental orders\",\n",
    "    \"It makes one to respect other people's views, culture and religion\",\n",
    "    \"The Electorate can check the excesses of the government through elections\"\n",
    "]\n",
    "\n",
    "def synthesize_text(text):\n",
    "    # This is a placeholder for the actual inference call.\n",
    "    # Many TTS models require a dedicated inference pipeline to convert model outputs to a waveform.\n",
    "    \n",
    "    # Here we simply tokenize and call model.generate(), then create a dummy waveform for illustration.\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = model.generate(**inputs)\n",
    "    \n",
    "    # In practice, replace the next lines with a function that converts model outputs into a mel-spectrogram and then to audio\n",
    "    sample_rate = 22050\n",
    "    waveform = torch.randn(1, sample_rate)  # Dummy waveform: 1 second of noise\n",
    "    return waveform, sample_rate\n",
    "\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    print(f\"Synthesizing audio for sentence {idx+1}...\")\n",
    "    waveform, sr = synthesize_text(sentence)\n",
    "    output_filename = f\"output_{idx+1}.wav\"\n",
    "    torchaudio.save(output_filename, waveform, sr)\n",
    "    print(f\"Saved {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Objective Evaluation\n",
    "\n",
    "- **Mel Cepstral Distortion (MCD):** Compare the spectral properties of the synthesized audio with reference samples.\n",
    "- **Signal-to-Noise Ratio (SNR):** Assess the quality of the generated waveform.\n",
    "\n",
    "### Subjective Evaluation\n",
    "\n",
    "- Conduct listening tests with native speakers to rate the naturalness, accent fidelity, and clarity.\n",
    "- Use Mean Opinion Score (MOS) tests to quantify the perceptual quality of the speech.\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "Compare generated outputs with any available ground-truth samples to identify pronunciation or prosody issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
